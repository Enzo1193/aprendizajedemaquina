{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":3114.96464,"end_time":"2025-06-24T18:36:41.727501","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-24T17:44:46.762861","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":2.171912,"end_time":"2025-06-24T17:44:55.419822","exception":false,"start_time":"2025-06-24T17:44:53.247910","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.Instalación de dependencias:\n### Prompt 1:\n\nQuiero comenzar un ejercicio de aprendizaje por refuerzo continuo utilizando el entorno Pendulum-v1 de Gymnasium junto con la librería TF-Agents. Para ello, necesito que primero instales las dependencias necesarias, incluyendo tf-agents[reverb] y gymnasium[classic-control]. Luego, importa todas las librerías relevantes para trabajar con TF-Agents, TensorFlow, NumPy, Matplotlib y Gym. A continuación, carga el entorno Pendulum-v1 usando suite_gym de TF-Agents para obtener dos entornos: uno para entrenamiento y otro para evaluación, y conviértelos a objetos TFPyEnvironment. También quiero que imprimas la especificación de observaciones y acciones del entorno de entrenamiento para verificar que todo está funcionando correctamente. Además, crea un entorno adicional usando Gymnasium (con render_mode=\"rgb_array\") solo para visualización, y define una función llamada plot_environment() que renderice una imagen del entorno y la muestre usando Matplotlib. Por favor, organiza el código resultante en celdas, como si fuera a ejecutarse en un entorno tipo Jupyter Notebook o Kaggle.","metadata":{"papermill":{"duration":0.002529,"end_time":"2025-06-24T17:44:55.425654","exception":false,"start_time":"2025-06-24T17:44:55.423125","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install tf-agents[reverb] gymnasium[classic-control]\n","metadata":{"papermill":{"duration":123.249425,"end_time":"2025-06-24T17:46:58.677796","exception":false,"start_time":"2025-06-24T17:44:55.428371","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.Importación de Librerias y Creacion del entorno","metadata":{"papermill":{"duration":0.010016,"end_time":"2025-06-24T17:46:58.700187","exception":false,"start_time":"2025-06-24T17:46:58.690171","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\n\nimport gymnasium as gym\n\n# Nombre del entorno\nenv_name = \"Pendulum-v1\"\n\n# Cargar entornos en modo Python (no-TensorFlow)\ntrain_py_env = suite_gym.load(env_name)\neval_py_env = suite_gym.load(env_name)\n\n# Convertir a entornos compatibles con TensorFlow (TFPyEnvironment)\ntrain_env = tf_py_environment.TFPyEnvironment(train_py_env)\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n# Mostrar especificaciones de observación y acción\nprint(\"Especificación de observaciones:\")\nprint(train_env.observation_spec())\n\nprint(\"\\nEspecificación de acciones:\")\nprint(train_env.action_spec())","metadata":{"papermill":{"duration":9.732444,"end_time":"2025-06-24T17:47:08.442852","exception":false,"start_time":"2025-06-24T17:46:58.710408","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.Entorno para visualización y función de renderizado","metadata":{"papermill":{"duration":0.010136,"end_time":"2025-06-24T17:47:08.463468","exception":false,"start_time":"2025-06-24T17:47:08.453332","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Crear entorno para visualización con modo gráfico\nrender_env = gym.make(env_name, render_mode=\"rgb_array\")\n\n# Función de visualización con Matplotlib\ndef plot_environment():\n    obs, _ = render_env.reset()\n    img = render_env.render()\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title(\"Visualización del entorno Pendulum-v1\")\n    plt.show()\n\n# Ejecutar visualización una vez\nplot_environment()\n","metadata":{"papermill":{"duration":0.336901,"end_time":"2025-06-24T17:47:08.810398","exception":false,"start_time":"2025-06-24T17:47:08.473497","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.Definición de hiperparametros para DDPG\n\n### Prompt 2:\nAhora necesito que definas los hiperparametros necesarios para entrenar un agente de DDPG en el entorno Pendulum-v1, usando TF-agents.","metadata":{"papermill":{"duration":0.010139,"end_time":"2025-06-24T17:47:08.831323","exception":false,"start_time":"2025-06-24T17:47:08.821184","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# -------------------------------------\n# Parámetros globales de entrenamiento\n# -------------------------------------\nnum_iterations = 100_000                # Número total de iteraciones de entrenamiento\ninitial_collect_steps = 1000            # Pasos de recolección antes del entrenamiento\ncollect_steps_per_iteration = 1         # Pasos de recolección por iteración\nreplay_buffer_capacity = 100_000        # Capacidad del buffer de replay\n\nbatch_size = 64                         # Tamaño del batch para entrenamiento\ncritic_learning_rate = 1e-3             # Tasa de aprendizaje del crítico\nactor_learning_rate = 1e-4              # Tasa de aprendizaje del actor\n\nlog_interval = 200                      # Frecuencia de impresión de logs\neval_interval = 1000                    # Frecuencia de evaluación\nnum_eval_episodes = 10                  # Número de episodios de evaluación\n\n# -------------------------------------\n# Parámetros de redes objetivo (target)\n# -------------------------------------\ntau = 0.005                             # Parámetro de actualización suave (soft update)\ngamma = 0.99                            # Factor de descuento (reward discount)\n\n# -------------------------------------\n# Arquitectura de redes\n# -------------------------------------\nactor_fc_layers = (400, 300)           # Capas ocultas del actor\ncritic_obs_fc_layers = ()              # Capas para observaciones en el crítico (ninguna adicional)\ncritic_action_fc_layers = ()           # Capas para acciones en el crítico (ninguna adicional)\ncritic_joint_fc_layers = (400, 300)    # Capas conjuntas observación-acción en el crítico\n\n# -------------------------------------\n# Parámetros de ruido Ornstein-Uhlenbeck para exploración\n# -------------------------------------\nou_stddev = 0.2                         # Desviación estándar del ruido OU\nou_damping = 0.15                       # Término de amortiguamiento\n","metadata":{"papermill":{"duration":0.016395,"end_time":"2025-06-24T17:47:08.858075","exception":false,"start_time":"2025-06-24T17:47:08.841680","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Definición de las redes Actor y Crítica\n### Prompt 3:\nSigue definiendo las redes neuronales necesarias para un agente DDPG (DdpgAgent) en el entorno Pendulum-v1 usando TF-Agents. Luego, define una red crítica que reciba como entrada una tupla (observación, acción). Usa los tamaños de capa definidos en los hiperparámetros. Al final, imprime una verificación que confirme que las redes fueron creadas correctamente.","metadata":{"papermill":{"duration":0.009949,"end_time":"2025-06-24T17:47:08.878097","exception":false,"start_time":"2025-06-24T17:47:08.868148","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.agents.ddpg.actor_network import ActorNetwork\nfrom tf_agents.agents.ddpg.critic_network import CriticNetwork\nfrom tf_agents.networks import utils\nfrom tf_agents.specs import tensor_spec\n\n# -------------------------------------\n# Especificaciones de entrada/salida\n# -------------------------------------\nobservation_spec = train_env.observation_spec()\naction_spec = train_env.action_spec()\ntime_step_spec = train_env.time_step_spec()\n\n# -------------------------------------\n# Red Actor: mapea observaciones → acciones\n# -------------------------------------\nactor_net = ActorNetwork(\n    input_tensor_spec=observation_spec,\n    output_tensor_spec=action_spec,\n    fc_layer_params=actor_fc_layers,\n    activation_fn=tf.keras.activations.relu,\n    name=\"ActorNetwork\"\n)\n\n# -------------------------------------\n# Red Crítica: mapea (observación, acción) → Q(s, a)\n# -------------------------------------\ncritic_net = CriticNetwork(\n    input_tensor_spec=(observation_spec, action_spec),\n    observation_fc_layer_params=critic_obs_fc_layers,\n    action_fc_layer_params=critic_action_fc_layers,\n    joint_fc_layer_params=critic_joint_fc_layers,\n    activation_fn=tf.keras.activations.relu,\n    name=\"CriticNetwork\"\n)\n\n# -------------------------------------\n# Verificación\n# -------------------------------------\nprint(\"✅ Red Actor creada:\", isinstance(actor_net, ActorNetwork))\nprint(\"✅ Red Crítico creada:\", isinstance(critic_net, CriticNetwork))\n","metadata":{"papermill":{"duration":0.280428,"end_time":"2025-06-24T17:47:09.168681","exception":false,"start_time":"2025-06-24T17:47:08.888253","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Creación del agente DDPG (DdpgAgent)\n### Prompt 4:\nAhora quiero que sigas con la creación del agente DdopgAgent usando estas redes creadas.","metadata":{"papermill":{"duration":0.012112,"end_time":"2025-06-24T17:47:09.191570","exception":false,"start_time":"2025-06-24T17:47:09.179458","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.agents.ddpg.ddpg_agent import DdpgAgent\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\n\n# -------------------------------------\n# Inicializadores (optimizers)\n# -------------------------------------\nactor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_learning_rate)\ncritic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_learning_rate)\n\n# -------------------------------------\n# Agente DDPG\n# -------------------------------------\nddpg_agent = DdpgAgent(\n    time_step_spec=train_env.time_step_spec(),\n    action_spec=action_spec,\n    actor_network=actor_net,\n    critic_network=critic_net,\n    actor_optimizer=actor_optimizer,\n    critic_optimizer=critic_optimizer,\n    ou_stddev=ou_stddev,\n    ou_damping=ou_damping,\n    target_update_tau=tau,\n    target_update_period=1,\n    gamma=gamma,\n    reward_scale_factor=1.0,\n    gradient_clipping=None,\n    train_step_counter=tf.Variable(0)\n)\n\n# Inicializar el agente (compilar redes y variables)\nddpg_agent.initialize()\n\n# Confirmación\nprint(\"✅ Agente DDPG creado y listo para entrenamiento.\")\n","metadata":{"papermill":{"duration":2.380569,"end_time":"2025-06-24T17:47:11.582296","exception":false,"start_time":"2025-06-24T17:47:09.201727","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Replay Buffer y Política de Recolección","metadata":{"papermill":{"duration":0.010267,"end_time":"2025-06-24T17:47:11.603504","exception":false,"start_time":"2025-06-24T17:47:11.593237","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.replay_buffers import tf_uniform_replay_buffer\n\n# -------------------------------------\n# Replay Buffer: para almacenar transiciones (s, a, r, s')\n# -------------------------------------\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n    data_spec=ddpg_agent.collect_data_spec,\n    batch_size=train_env.batch_size,\n    max_length=replay_buffer_capacity\n)\n\n# -------------------------------------\n# Política de recolección (usa directamente la del agente DDPG con OU ya incluido)\n# -------------------------------------\ncollect_policy = ddpg_agent.collect_policy\n\n# -------------------------------------\n# Confirmación\n# -------------------------------------\nprint(\"✅ Replay Buffer inicializado con capacidad =\", replay_buffer_capacity)\nprint(\"✅ Política de recolección (con OU) del agente configurada.\")\n","metadata":{"papermill":{"duration":0.040098,"end_time":"2025-06-24T17:47:11.654492","exception":false,"start_time":"2025-06-24T17:47:11.614394","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Driver para la recolección inicial de datos","metadata":{"papermill":{"duration":0.011086,"end_time":"2025-06-24T17:47:11.676181","exception":false,"start_time":"2025-06-24T17:47:11.665095","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.trajectories import trajectory\n\n# -------------------------------------\n# Métricas opcionales de recolección\n# -------------------------------------\nenv_steps_metric = tf_metrics.EnvironmentSteps()\navg_return_metric = tf_metrics.AverageReturnMetric()\n\n# -------------------------------------\n# Driver para recolección de pasos (experiencias iniciales)\n# -------------------------------------\ninitial_collect_driver = DynamicStepDriver(\n    env=train_env,\n    policy=collect_policy,\n    observers=[replay_buffer.add_batch, env_steps_metric],\n    num_steps=initial_collect_steps\n)\n\n# Ejecutar recolección inicial\ninitial_collect_driver.run()\n\nprint(f\"✅ Recolección inicial completada con {initial_collect_steps} pasos.\")\n","metadata":{"papermill":{"duration":11.192752,"end_time":"2025-06-24T17:47:22.880116","exception":false,"start_time":"2025-06-24T17:47:11.687364","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Entrenamiento del agente DDPG","metadata":{"papermill":{"duration":0.010516,"end_time":"2025-06-24T17:47:22.946180","exception":false,"start_time":"2025-06-24T17:47:22.935664","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.utils.common import function\nfrom tqdm import trange\n\n# -------------------------------------\n# Dataset del buffer para muestreo\n# -------------------------------------\ndataset = replay_buffer.as_dataset(\n    num_parallel_calls=tf.data.AUTOTUNE,\n    sample_batch_size=batch_size,\n    num_steps=2\n).prefetch(tf.data.AUTOTUNE)\n\niterator = iter(dataset)\n\n# -------------------------------------\n# Entrenamiento en modo tf.function\n# -------------------------------------\nddpg_agent.train = function(ddpg_agent.train)\n\n# -------------------------------------\n# Métricas para seguimiento\n# -------------------------------------\ntrain_rewards = []\n\nprint(\"🚀 Entrenando agente DDPG...\\n\")\n\nfor step in trange(num_iterations):\n    # Recolectar nuevos pasos\n    time_step = train_env.current_time_step()\n    action = collect_policy.action(time_step)\n    next_time_step = train_env.step(action.action)\n    traj = trajectory.from_transition(time_step, action, next_time_step)\n    replay_buffer.add_batch(traj)\n\n    # Entrenar con muestra del buffer\n    experience, _ = next(iterator)\n    train_loss = ddpg_agent.train(experience).loss\n\n    # Evaluación periódica\n    if step % eval_interval == 0:\n        total_return = 0.0\n        for _ in range(num_eval_episodes):\n            time_step = eval_env.reset()\n            episode_return = 0.0\n            while not time_step.is_last():\n                action = ddpg_agent.policy.action(time_step)\n                time_step = eval_env.step(action.action)\n                episode_return += time_step.reward.numpy()[0]\n            total_return += episode_return\n        avg_return = total_return / num_eval_episodes\n        train_rewards.append(avg_return)\n        print(f\"Iteración {step}: Recompensa promedio = {avg_return:.2f}\")\n","metadata":{"papermill":{"duration":2954.60247,"end_time":"2025-06-24T18:36:37.559276","exception":false,"start_time":"2025-06-24T17:47:22.956806","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}