{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":3114.96464,"end_time":"2025-06-24T18:36:41.727501","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-24T17:44:46.762861","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":2.171912,"end_time":"2025-06-24T17:44:55.419822","exception":false,"start_time":"2025-06-24T17:44:53.247910","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1.Instalaci√≥n de dependencias:\n### Prompt 1:\n\nQuiero comenzar un ejercicio de aprendizaje por refuerzo continuo utilizando el entorno Pendulum-v1 de Gymnasium junto con la librer√≠a TF-Agents. Para ello, necesito que primero instales las dependencias necesarias, incluyendo tf-agents[reverb] y gymnasium[classic-control]. Luego, importa todas las librer√≠as relevantes para trabajar con TF-Agents, TensorFlow, NumPy, Matplotlib y Gym. A continuaci√≥n, carga el entorno Pendulum-v1 usando suite_gym de TF-Agents para obtener dos entornos: uno para entrenamiento y otro para evaluaci√≥n, y convi√©rtelos a objetos TFPyEnvironment. Tambi√©n quiero que imprimas la especificaci√≥n de observaciones y acciones del entorno de entrenamiento para verificar que todo est√° funcionando correctamente. Adem√°s, crea un entorno adicional usando Gymnasium (con render_mode=\"rgb_array\") solo para visualizaci√≥n, y define una funci√≥n llamada plot_environment() que renderice una imagen del entorno y la muestre usando Matplotlib. Por favor, organiza el c√≥digo resultante en celdas, como si fuera a ejecutarse en un entorno tipo Jupyter Notebook¬†o¬†Kaggle.","metadata":{"papermill":{"duration":0.002529,"end_time":"2025-06-24T17:44:55.425654","exception":false,"start_time":"2025-06-24T17:44:55.423125","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install tf-agents[reverb] gymnasium[classic-control]\n","metadata":{"papermill":{"duration":123.249425,"end_time":"2025-06-24T17:46:58.677796","exception":false,"start_time":"2025-06-24T17:44:55.428371","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.Importaci√≥n de Librerias y Creacion del entorno","metadata":{"papermill":{"duration":0.010016,"end_time":"2025-06-24T17:46:58.700187","exception":false,"start_time":"2025-06-24T17:46:58.690171","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\n\nimport gymnasium as gym\n\n# Nombre del entorno\nenv_name = \"Pendulum-v1\"\n\n# Cargar entornos en modo Python (no-TensorFlow)\ntrain_py_env = suite_gym.load(env_name)\neval_py_env = suite_gym.load(env_name)\n\n# Convertir a entornos compatibles con TensorFlow (TFPyEnvironment)\ntrain_env = tf_py_environment.TFPyEnvironment(train_py_env)\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n# Mostrar especificaciones de observaci√≥n y acci√≥n\nprint(\"Especificaci√≥n de observaciones:\")\nprint(train_env.observation_spec())\n\nprint(\"\\nEspecificaci√≥n de acciones:\")\nprint(train_env.action_spec())","metadata":{"papermill":{"duration":9.732444,"end_time":"2025-06-24T17:47:08.442852","exception":false,"start_time":"2025-06-24T17:46:58.710408","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.Entorno para visualizaci√≥n y funci√≥n de renderizado","metadata":{"papermill":{"duration":0.010136,"end_time":"2025-06-24T17:47:08.463468","exception":false,"start_time":"2025-06-24T17:47:08.453332","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Crear entorno para visualizaci√≥n con modo gr√°fico\nrender_env = gym.make(env_name, render_mode=\"rgb_array\")\n\n# Funci√≥n de visualizaci√≥n con Matplotlib\ndef plot_environment():\n    obs, _ = render_env.reset()\n    img = render_env.render()\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title(\"Visualizaci√≥n del entorno Pendulum-v1\")\n    plt.show()\n\n# Ejecutar visualizaci√≥n una vez\nplot_environment()\n","metadata":{"papermill":{"duration":0.336901,"end_time":"2025-06-24T17:47:08.810398","exception":false,"start_time":"2025-06-24T17:47:08.473497","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.Definici√≥n de hiperparametros para DDPG\n\n### Prompt 2:\nAhora necesito que definas los hiperparametros necesarios para entrenar un agente de DDPG en el entorno Pendulum-v1, usando TF-agents.","metadata":{"papermill":{"duration":0.010139,"end_time":"2025-06-24T17:47:08.831323","exception":false,"start_time":"2025-06-24T17:47:08.821184","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# -------------------------------------\n# Par√°metros globales de entrenamiento\n# -------------------------------------\nnum_iterations = 100_000                # N√∫mero total de iteraciones de entrenamiento\ninitial_collect_steps = 1000            # Pasos de recolecci√≥n antes del entrenamiento\ncollect_steps_per_iteration = 1         # Pasos de recolecci√≥n por iteraci√≥n\nreplay_buffer_capacity = 100_000        # Capacidad del buffer de replay\n\nbatch_size = 64                         # Tama√±o del batch para entrenamiento\ncritic_learning_rate = 1e-3             # Tasa de aprendizaje del cr√≠tico\nactor_learning_rate = 1e-4              # Tasa de aprendizaje del actor\n\nlog_interval = 200                      # Frecuencia de impresi√≥n de logs\neval_interval = 1000                    # Frecuencia de evaluaci√≥n\nnum_eval_episodes = 10                  # N√∫mero de episodios de evaluaci√≥n\n\n# -------------------------------------\n# Par√°metros de redes objetivo (target)\n# -------------------------------------\ntau = 0.005                             # Par√°metro de actualizaci√≥n suave (soft update)\ngamma = 0.99                            # Factor de descuento (reward discount)\n\n# -------------------------------------\n# Arquitectura de redes\n# -------------------------------------\nactor_fc_layers = (400, 300)           # Capas ocultas del actor\ncritic_obs_fc_layers = ()              # Capas para observaciones en el cr√≠tico (ninguna adicional)\ncritic_action_fc_layers = ()           # Capas para acciones en el cr√≠tico (ninguna adicional)\ncritic_joint_fc_layers = (400, 300)    # Capas conjuntas observaci√≥n-acci√≥n en el cr√≠tico\n\n# -------------------------------------\n# Par√°metros de ruido Ornstein-Uhlenbeck para exploraci√≥n\n# -------------------------------------\nou_stddev = 0.2                         # Desviaci√≥n est√°ndar del ruido OU\nou_damping = 0.15                       # T√©rmino de amortiguamiento\n","metadata":{"papermill":{"duration":0.016395,"end_time":"2025-06-24T17:47:08.858075","exception":false,"start_time":"2025-06-24T17:47:08.841680","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Definici√≥n de las redes Actor y Cr√≠tica\n### Prompt 3:\nSigue definiendo las redes neuronales necesarias para un agente DDPG (DdpgAgent) en el entorno Pendulum-v1 usando TF-Agents. Luego, define una red cr√≠tica que reciba como entrada una tupla (observaci√≥n, acci√≥n). Usa los tama√±os de capa definidos en los hiperpar√°metros. Al final, imprime una verificaci√≥n que confirme que las redes fueron creadas¬†correctamente.","metadata":{"papermill":{"duration":0.009949,"end_time":"2025-06-24T17:47:08.878097","exception":false,"start_time":"2025-06-24T17:47:08.868148","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.agents.ddpg.actor_network import ActorNetwork\nfrom tf_agents.agents.ddpg.critic_network import CriticNetwork\nfrom tf_agents.networks import utils\nfrom tf_agents.specs import tensor_spec\n\n# -------------------------------------\n# Especificaciones de entrada/salida\n# -------------------------------------\nobservation_spec = train_env.observation_spec()\naction_spec = train_env.action_spec()\ntime_step_spec = train_env.time_step_spec()\n\n# -------------------------------------\n# Red Actor: mapea observaciones ‚Üí acciones\n# -------------------------------------\nactor_net = ActorNetwork(\n    input_tensor_spec=observation_spec,\n    output_tensor_spec=action_spec,\n    fc_layer_params=actor_fc_layers,\n    activation_fn=tf.keras.activations.relu,\n    name=\"ActorNetwork\"\n)\n\n# -------------------------------------\n# Red Cr√≠tica: mapea (observaci√≥n, acci√≥n) ‚Üí Q(s, a)\n# -------------------------------------\ncritic_net = CriticNetwork(\n    input_tensor_spec=(observation_spec, action_spec),\n    observation_fc_layer_params=critic_obs_fc_layers,\n    action_fc_layer_params=critic_action_fc_layers,\n    joint_fc_layer_params=critic_joint_fc_layers,\n    activation_fn=tf.keras.activations.relu,\n    name=\"CriticNetwork\"\n)\n\n# -------------------------------------\n# Verificaci√≥n\n# -------------------------------------\nprint(\"‚úÖ Red Actor creada:\", isinstance(actor_net, ActorNetwork))\nprint(\"‚úÖ Red Cr√≠tico creada:\", isinstance(critic_net, CriticNetwork))\n","metadata":{"papermill":{"duration":0.280428,"end_time":"2025-06-24T17:47:09.168681","exception":false,"start_time":"2025-06-24T17:47:08.888253","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Creaci√≥n del agente DDPG (DdpgAgent)\n### Prompt 4:\nAhora quiero que sigas con la creaci√≥n del agente DdopgAgent usando estas redes creadas.","metadata":{"papermill":{"duration":0.012112,"end_time":"2025-06-24T17:47:09.191570","exception":false,"start_time":"2025-06-24T17:47:09.179458","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.agents.ddpg.ddpg_agent import DdpgAgent\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\n\n# -------------------------------------\n# Inicializadores (optimizers)\n# -------------------------------------\nactor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_learning_rate)\ncritic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_learning_rate)\n\n# -------------------------------------\n# Agente DDPG\n# -------------------------------------\nddpg_agent = DdpgAgent(\n    time_step_spec=train_env.time_step_spec(),\n    action_spec=action_spec,\n    actor_network=actor_net,\n    critic_network=critic_net,\n    actor_optimizer=actor_optimizer,\n    critic_optimizer=critic_optimizer,\n    ou_stddev=ou_stddev,\n    ou_damping=ou_damping,\n    target_update_tau=tau,\n    target_update_period=1,\n    gamma=gamma,\n    reward_scale_factor=1.0,\n    gradient_clipping=None,\n    train_step_counter=tf.Variable(0)\n)\n\n# Inicializar el agente (compilar redes y variables)\nddpg_agent.initialize()\n\n# Confirmaci√≥n\nprint(\"‚úÖ Agente DDPG creado y listo para entrenamiento.\")\n","metadata":{"papermill":{"duration":2.380569,"end_time":"2025-06-24T17:47:11.582296","exception":false,"start_time":"2025-06-24T17:47:09.201727","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Replay Buffer y Pol√≠tica de Recolecci√≥n","metadata":{"papermill":{"duration":0.010267,"end_time":"2025-06-24T17:47:11.603504","exception":false,"start_time":"2025-06-24T17:47:11.593237","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.replay_buffers import tf_uniform_replay_buffer\n\n# -------------------------------------\n# Replay Buffer: para almacenar transiciones (s, a, r, s')\n# -------------------------------------\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n    data_spec=ddpg_agent.collect_data_spec,\n    batch_size=train_env.batch_size,\n    max_length=replay_buffer_capacity\n)\n\n# -------------------------------------\n# Pol√≠tica de recolecci√≥n (usa directamente la del agente DDPG con OU ya incluido)\n# -------------------------------------\ncollect_policy = ddpg_agent.collect_policy\n\n# -------------------------------------\n# Confirmaci√≥n\n# -------------------------------------\nprint(\"‚úÖ Replay Buffer inicializado con capacidad =\", replay_buffer_capacity)\nprint(\"‚úÖ Pol√≠tica de recolecci√≥n (con OU) del agente configurada.\")\n","metadata":{"papermill":{"duration":0.040098,"end_time":"2025-06-24T17:47:11.654492","exception":false,"start_time":"2025-06-24T17:47:11.614394","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Driver para la recolecci√≥n inicial de datos","metadata":{"papermill":{"duration":0.011086,"end_time":"2025-06-24T17:47:11.676181","exception":false,"start_time":"2025-06-24T17:47:11.665095","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.trajectories import trajectory\n\n# -------------------------------------\n# M√©tricas opcionales de recolecci√≥n\n# -------------------------------------\nenv_steps_metric = tf_metrics.EnvironmentSteps()\navg_return_metric = tf_metrics.AverageReturnMetric()\n\n# -------------------------------------\n# Driver para recolecci√≥n de pasos (experiencias iniciales)\n# -------------------------------------\ninitial_collect_driver = DynamicStepDriver(\n    env=train_env,\n    policy=collect_policy,\n    observers=[replay_buffer.add_batch, env_steps_metric],\n    num_steps=initial_collect_steps\n)\n\n# Ejecutar recolecci√≥n inicial\ninitial_collect_driver.run()\n\nprint(f\"‚úÖ Recolecci√≥n inicial completada con {initial_collect_steps} pasos.\")\n","metadata":{"papermill":{"duration":11.192752,"end_time":"2025-06-24T17:47:22.880116","exception":false,"start_time":"2025-06-24T17:47:11.687364","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Entrenamiento del agente DDPG","metadata":{"papermill":{"duration":0.010516,"end_time":"2025-06-24T17:47:22.946180","exception":false,"start_time":"2025-06-24T17:47:22.935664","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tf_agents.utils.common import function\nfrom tqdm import trange\n\n# -------------------------------------\n# Dataset del buffer para muestreo\n# -------------------------------------\ndataset = replay_buffer.as_dataset(\n    num_parallel_calls=tf.data.AUTOTUNE,\n    sample_batch_size=batch_size,\n    num_steps=2\n).prefetch(tf.data.AUTOTUNE)\n\niterator = iter(dataset)\n\n# -------------------------------------\n# Entrenamiento en modo tf.function\n# -------------------------------------\nddpg_agent.train = function(ddpg_agent.train)\n\n# -------------------------------------\n# M√©tricas para seguimiento\n# -------------------------------------\ntrain_rewards = []\n\nprint(\"üöÄ Entrenando agente DDPG...\\n\")\n\nfor step in trange(num_iterations):\n    # Recolectar nuevos pasos\n    time_step = train_env.current_time_step()\n    action = collect_policy.action(time_step)\n    next_time_step = train_env.step(action.action)\n    traj = trajectory.from_transition(time_step, action, next_time_step)\n    replay_buffer.add_batch(traj)\n\n    # Entrenar con muestra del buffer\n    experience, _ = next(iterator)\n    train_loss = ddpg_agent.train(experience).loss\n\n    # Evaluaci√≥n peri√≥dica\n    if step % eval_interval == 0:\n        total_return = 0.0\n        for _ in range(num_eval_episodes):\n            time_step = eval_env.reset()\n            episode_return = 0.0\n            while not time_step.is_last():\n                action = ddpg_agent.policy.action(time_step)\n                time_step = eval_env.step(action.action)\n                episode_return += time_step.reward.numpy()[0]\n            total_return += episode_return\n        avg_return = total_return / num_eval_episodes\n        train_rewards.append(avg_return)\n        print(f\"Iteraci√≥n {step}: Recompensa promedio = {avg_return:.2f}\")\n","metadata":{"papermill":{"duration":2954.60247,"end_time":"2025-06-24T18:36:37.559276","exception":false,"start_time":"2025-06-24T17:47:22.956806","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}